---
title: "Assigment - Naive Bayes DIY - Authored"
author:
  - Scott Erkan - 617570 - Author
  - Emiel Maters 634735 - Reviewer
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```

## Business Understanding

Nowadays everyone is able to access the internet very easily, and it's just as easy to share information with others. Whether this information is factual or simply bullshit. Today we will use the Naive Bayes Classifier (a classifier is a machine learning model that is used to discriminate different objects based on certain features) in order to create a model that is able to identify fake news articles.

Alright, let's get right into it by importing our data and creating a data frame.

## Data Understanding

```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawDF <- read.csv(url)
head(rawDF)
```

Now that we have our data frame we have to make sure the labels are seen as two clear categories: "fake" and "confirmed" (meaning factual).

**Review: changed type to label**

```{r}

# Firstly, we convert the characters into a factor variable. This can be done using 1 and 2. The class function should show us that it is now indeed a factor variable.

rawDF$label <- rawDF$label %>% factor %>% relevel("1")
class(rawDF$label)

rawDF$label <- rawDF$label %>% factor %>% relevel("0")
class(rawDF$label) 

# Secondly, we assign the categories.

fake <- rawDF %>% filter(label == "1")
confirmed <- rawDF %>% filter(label == "0")

# Thirdly, we can make a quick visualization using word clouds. Maybe we can already notice a difference in word usage in fake and confirmed news.

wordcloud(fake$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(confirmed$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))

```

## Data Preparation

Here we create a corpus (a collection of text documents) after which we also eliminate all items from our data set that will add little to no information to our model.

```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1:3])

cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
```

Time to check how the clean data looks compared to the raw data after our changes.

**Review: tribble = tibble**

```{r}
tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])
```

We can also transform the messages into a matrix.

```{r}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```

Before we start modeling we need to split our data into test- and training sets.

**Review: lst=TRUE --> list=FALSE**

```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

# Apply split indices to DF
trainDF <- rawDF[trainIndex, ]
testDF <- rawDF[-trainIndex, ]

# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]
```

Now we reduce features to save on computation time.

**Review: increased number to prevent my laptop from crashing**
**Review: MatrixDocumentTerm --> DocumentTermMatrix**

```{r}
freqWords <- trainDTM %>% findFreqTerms(1000)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))
```

"We will transform the counts into a factor that simply indicates whether the word appears in the document or not. We'll first build our own function for this and then apply it to each column in the DTM."


**Review: MatrixDocumentTerm --> DocumentTermMatrix**

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```

## Modeling

It's modeling time!

**Review: changed predict line to predict(nbayesModel, testDTM)**
**Review: FAKE --> 1**

```{r}
nbayesModel <- naiveBayes(trainDTM, trainDF$label, laplace = 1)
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "1", dnn = c("Prediction", "True"))
```

## Evaluation and Deployment

The model has an accuracy of around 78%. This is not bad, though somewhere above the 90% would be ideal. Multiple word frequencies were tested (5, 100, 200, 5000 & 7000) without significant changes in accuracy. The model can be used for solving multi-class prediction problems. It is relatively fast and does not require a lot of training data, but it assumes that features are independent of each other. For this reason it is not suited for all situations, and depending on the data more complex algorithms should be considered.

## Reviewer adds suggestions for improving the model

The model is well written. Above each code chuck, scott explains what the complete chunk will do. Sometimes he also explains per piece in a chunk, but not everywhere. This could be an improvement. This way persons with no knowledge about NB can know what the code does.
